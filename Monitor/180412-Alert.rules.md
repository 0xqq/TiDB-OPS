---
title: AlertManager 进阶操作 - 生产环境应用
date: 2018-03-23 15:34:01
updated: 2018-04-12 13:49:26
categories:
  - Monitor
tags:
  - Deploy
  - AlertManager
  - Proemtheus
---
# AlertManager 进阶操作 - 生产环境应用

## Alert.rule Labels 定义与配置样例

- labels 定义
  - 参考: [DATA STRUCTURES](https://prometheus.io/docs/alerting/notifications/)
  - 参考: [Custom Alertmanager Templates](https://prometheus.io/blog/2016/03/03/custom-alertmanager-templates/)
  - 参考: [NOTIFICATION TEMPLATE EXAMPLES](https://prometheus.io/docs/alerting/notification_examples/)
  - 参考: [METRIC AND LABEL NAMING](https://prometheus.io/docs/practices/naming/#labels)
- Alert.rules 参数定义 [引用站外文章](http://dockone.io/article/2098)
  - "Alert" 是报警规则的名字，名字间不能有空格，可以用下划线链接；
  - "IF" 是数据的查询表达式
  - "FOR" 是报警状态持续超过 1 分钟后，将报警由状态 "PENDING" 改为 "FIRING"，报警将交给 Alertmanager 处理。
  - "LABELS" 为自定义数据，我们在这里指定了报警的级别和显示 "IF" 中表达式的值。
  - "ANNOTATIONS" 为自定义数据，我们在这里提供报警的现象和原因介绍。

- LABELS
  - env
    - test-cluster
  - channels
    - alerts
  - level
    - emergency  0     phone
    - critical   1     sms
    - warning    2     wechat
    - notice     3     slack
  - service
    - TiDB
    - TiKV
    - PD
    - OS
  - color
    - #2eb886

### Alert.rule Labels Example

- 在 Prometheus 配置文件中引用相关告警规则，重启后生效

  ```YAML
  rule_files:
    - 'test.lablels.yml'
  ```

- 相关内容添加到 `test.lablels.yml` 文件中

  ```YAML
  groups:
  - name: alert.rules
    rules:
    - alert: NODE_disk_readonly
      expr: node_filesystem_readonly{fstype=~"(ext.|xfs)"}  > 1
      for: 1m
      labels:
        env: test-cluster
        level: emergency
        service: OS
        channels: alerts
        expr: node_filesystem_readonly{fstype=~"(ext.|xfs)"}  > 1
      annotations:
        description: 'alert:{{ $labels.expr }} instance:
          {{ $labels.instance }} values: {{ $value }}'
        value: '{{ $value }}'
        summary: disk readonly

    - alert: TiDB_memery_abnormal
      expr: go_memstats_heap_inuse_bytes{job="tidb"} > 1e+10
      for: 1m
      labels:
        env: test-cluster
        level: warning
        service: TiDB
        channels: alerts
        expr: go_memstats_heap_inuse_bytes{job="tidb"} > 1e+10
      annotations:
        description: 'alert: {{ $labels.expr }}  values: {{ $value }}'
        value: '{{ $value }}'
        summary: TiDB mem heap is over 1GiB

    - alert: PD_etcd_write_disk_latency
      expr: histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[1m])) by (instance,job,le) ) > 1
      for: 1m
      labels:
        env: test-cluster
        level: emergency
        service: PD
        channels: alerts
        expr:  histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[1m])) by (instance,job,le) ) > 1
      annotations:
        description: 'alert: {{ $labels.expr }} instance: {{ $labels.instance }}   values:{{ $value }}'
        value: '{{ $value }}'
        summary: PD_etcd_write_disk_latency

    - alert: TiKV_server_report_failure_msg_total
      expr:  sum(rate(tikv_server_report_failure_msg_total{type="unreachable"}[10m])) BY (store_id) > 0
      for: 1m
      labels:
        env: test-cluster
        level: critical
        service: TiKV
        channels: alerts
        color: 2eb886
        expr:  sum(rate(tikv_server_report_failure_msg_total{type="unreachable"}[10m])) BY (store_id) > 0
      annotations:
        description: 'alert: {{ $labels.expr }} instance: {{ $labels.instance }} values: {{ $value }}'
        value: '{{ $value }}'
        summary: TiKV server_report_failure_msg_total error
  ```

## AlertManager to Slack 告警案例

- slack_config 配置测试
  - 参考: [slack_config](https://prometheus.io/docs/alerting/configuration/#slack_config)
  - 参考: [slack_api](https://api.slack.com/docs/message-attachments)

- AlertManager config 示例
  - [Example](https://github.com/prometheus/alertmanager#example)

- 以下内容保存为 `alertmanager.yml` 文件，AlertManager 启动时指定该配置文件启动

  ```YAML
  global:
    slack_api_url: 'https://hooks.slack.com/services/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'
    # slack api 地址
  route:
    receiver: "alerts-test"
    group_by: ['env','instance']
    group_wait:      30s
    group_interval:  3m
    repeat_interval: 3m
    routes:
    - match:
        env: test-cluster
      receiver: alerts
      continue: true
    - match:
        env: test-cluster  #同时满足 env:test-cluster、severity:critical、server: TiKV、channels: alerts 才会发送到 receiver: labels-test
        severity: critical
        server: TiKV
        channels: alerts
      receiver: labels-test
  # 默认 接收器
  receivers:
  - name: 'alerts-test'
    slack_configs:
    - channel: '#alters-test'
      username: 'default'
      icon_emoji: ':innocent:'
      title:   '{{.CommonAnnotations.summary}}'
      text:    'in {{.CommonLabels.env}}: {{ .CommonLabels.alertname }}  {{ .CommonAnnotations.description }}'
  # 普通版
  - name: 'alerts'
    slack_configs:
    - channel: '#long'
      username: 'alert'
      icon_emoji: ':TiDB:'
      title:   '{{.CommonAnnotations.summary}} : {{ .CommonLabels.level}}'
      text:    'in {{.CommonLabels.env}}: {{ .GroupLabels.alertstat }} : {{ .CommonAnnotations.description }}'
  # labels (参考 slack_api 与 slack_config)
  - name: 'labels-test'
    slack_configs:
    - channel: '#{{.CommonLabels.channels}}'
      fallback: '{{.CommonLabels.alertname}} : {{ .CommonAnnotations.summary }}'
      username: 'Robot'
      icon_emoji: ':joy:'
      icon_url: 'http://www.pingcap.com'
      color: '#{{.CommonLabels.color}}' #改变 Slack 文本框颜色
      title:  '{{.CommonLabels.alertname}} : {{ .CommonAnnotations.summary }} : {{ .CommonLabels.server}} : {{ .CommonLabels.level}}'
      title_link: 'http://172.16.10.65:9090/alerts'
      text:   'in {{.CommonLabels.env}} : {{ .CommonAnnotations.description }}'
      pretext: 'http://wiki.pingcap.com/{{.CommonLabels.srv}}/{{ .CommonLabels.alertname}}'
  ```

### 启动 AlertManager

- alertmanager 启动参数
  - `./alertmanager -config.file=alertmanager.yml -storage.path=./data/ -log.level=debug -web.listen-address=0.0.0.0:9093 -data.retention=1h -log.format="logger:syslog?appname=alert-rules-test&local=7"`
  - log 输出到 `/var/log/messages` , 搜索 `appname=alert-rules-test`，用于告警调试使用

### 一条告警告警发送过程

- 告警发送经过
  1. Prometheus 根据 alert.rules 匹配 metric 数据，匹配成功后发送到 AlertManager
  1. AlertManager 收到后，一条告警会通过 `route > match > LABELS > receivers` 匹配到接收器
  1. 然后根据 group_by 内容匹配并聚合告警

- group_by: ['env','instance'] 参数说明
  - env and instance 划分为一组，如果有多个参数，递归匹配，直到匹配不到
  - 取值范围 `alert.rule > LABELS` 、`metrics > instance、job、type` 等

## API 信息

- [http api 说明](https://prometheus.io/docs/querying/api/)
- 请求 Prometheus alert 告警 json 格式
  - 使用前需要编码
  - `http://172.16.10.65:9090/api/v1/query?query=(100%20-%20(avg%20by%20(instance)%20(irate(node_cpu%7Bmode%3D%22idle%22%7D%5B5m%5D))%20*%20100))%20&step=15s`
    - 请求 cpu 使用量
  - URL 编码 `URL-encoded`
- 请求 alertmanager 告警 json 格式
  - `curl 'http://localhost:9093/api/v1/alerts'`